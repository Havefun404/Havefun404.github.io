---
title: GPT系列技术演进
date: 2023-01-28 22:02:09
katex: true
tags:
---
GPT1-GPT3系列
<!--more-->

# 大模型演进之路
## 概览
随着chatGPT史诗级别的破圈，AI智能所演化出的产品如雨后春笋般，进入各行各业人士的眼中。无论是专业人士，还是b站up主，无一不被chatGPT的强大与智能所折服。而这一切的一切终究要追溯到**OpenAI**——这家正在改变未来的科研机构，在语言智能领域上长年累月的探索和坚持。经过一代又一代的演变之后：
+ 一方面语言模型在几乎大部分的NLP任务上都有效果。
+ 另一方面根据人们的需求生成更有帮助的内容，变成众人可测，门槛更低的产品。

若想要深入了解chatGPT，那就必须追寻其背后一代又一代的技术变革。

![演变之路](LLM.png)
<!-- {% asset_img ./GPT/LLM.png 演变之路 %} -->
***

## GPT：奠定AR框架路径，预训练+fine-tune先河

**GPT(Generative Pre-Training)**在Transformer之后提出，虽然GPT1先于举世闻名的BERT提出，尽管GPT1提出了大家目前最为熟悉的预训练+微调的模式，但奈何BERT的效果如开挂一般，横扫千军。但世人均不可否认GPT对语言智能的贡献，以及后续陆续演化出的**巨无霸——GPT3**。

受到Transformer的启发，GPT系列模型的主要架构都是Transformer中的decoder架构，通常将这种架构归类为自回归类**AR(Autoregressive)**

> AR框架：单向获取信息，只能向前或向后来预测指定位置单词，e.g. GPT, ELMO

- AR优点： 适合长生成类任务
- AR缺点： 无法获取双向信息

> AE框架：双向获取信息，向前和向后双向，e.g. BERT

- AE优点： 利用双向信息
- AE缺点： 预训练与微调gap

### 利用无标注数据预训练

模型上GPT只采用了Transformer的解码器部分，即模型只能看到当前词元和之前的信息，后面的信息都会被mask。同时，使用一个窗口大小**k**来预测该窗口的下一个词，目标函数如下：


![alt](https://latex.vimsky.com/test.image.latex.php?fmt=svg&val=%255Cdpi%257B150%257D%2520%255Clarge%2520L_1%2528u%2529%2520%253D%2520%255Csum_i%2520logP%2528u_i%255C%2520%257C%255C%2520u_%257Bi-k%257D%2520%255C%2520...%255C%2520u_%257Bi-1%257D%253B%2520%255Ctheta%2529&dl=0)

其中，代表第一个损失函数；$u$代表词元；k代表窗口大小；$\theta$代表模型参数。

### 利用标号数据Finetune

在Finetune过程中，模型利用新的有标号的数据重新进行训练。将输入送给模型后，得到最终transformer模块的输出${h_l^m}$，将该部分输入到一层linear，最终放入softmax进行预测，该部分的目标函数如下：

![alt ](https://latex.vimsky.com/test.image.latex.php?fmt=svg&val=%255Cdpi%257B150%257D%2520%255Clarge%2520L_2%2528C%2529%2520%253D%2520%255Csum_%257B%2528x%252Cy%2529%257DlogP%2528y%255C%2520%257C%255C%2520x%255E1%252C%255C%2520...%255C%2520%252Cx%255Em%2529&dl=0)

同时，在Finetune中，作者发现将语言模型作为辅助的目标函数，有助于泛化性和更便于收敛。因此，最终优化后的目标函数变为：

![alt ](https://latex.vimsky.com/test.image.latex.php?fmt=svg&val=%255Cdpi%257B150%257D%2520%255Clarge%2520L_3%2528C%2529%2520%253D%2520L_1%2528C%2529%2520%26plus%3B%255Clambda%2520L_2%2528C%2529&dl=0)

而对于不同的任务，Finetune时会加上任务特定的linear层，来表示不同的input和label，如下图：

![alt GPT Finetune](finetune.png)

总的来说，GPT所应用的数据(BookCorpus)和其参数规模(0.117B)与后续的模型有一定差距，并且效果被强力后辈**Bert**狠狠碾压，效果并没有非常出色。
***
## GPT-2 更大规模，Prompt助力

此时，已是Bert的时代，Bert声名大噪，无论是模型规模还是在下游任务中效果都产生了巨大的影响。而Bert采用的AE框架也体现出了自己的优势，毕竟效果太过显著。但OpenAI这样的公司，怎会放弃自己的老路，成为Bert的“阶下囚”，随后推出了GPT2模型。

GPT2主要的两个工作为：
- 扩大规模：数据上，使用筛选了更优质的数据；模型上，将参数扩大到1.5B
- 更自然的融合方式，下面重点介绍这点。

## zero-shot 与 Prompt
无论是GPT，还是Bert都采用了预训练+微调的方式。但：
- 预训练的过程和微调的下游任务之间可能存在一定的gap，无法完全发挥预训练模型的优势；
- 同时微调的时候也需要将大模型重新训练，耗时耗力；
- 并且在微调时需要收集带标签的数据。

为了克服这些困难，GPT2利用**Prompt**的方式，将下游的任务加到预训练模型中进行预测，从而彻底丢弃微调训练的过程，进而实现**zero-shot**。
Prompt具体加入方式，举例来说，在机器翻译中，将句子表示成：
<u>**Translate it**</u> ,<u>I love computer</u> ,<u>我爱计算机</u> 开头的部分就为Prompt。

引入Prompt起到作用的原因，可以大致理解为：如果模型足够强大就有能力识别出要求；其次数据中可能存在类似prompt这样的信息。尽管GPT2在不少任务上有效果，但相比Bert而言并没有太大的提升。虽然GPT2并没有激起多大的风浪，但其更自然的融合方式，这样的思想为后续的GPT3以及如今的chatGPT打下来重要的基石。

***

## GPT-3 大力出奇迹

GPT-2的zero-shot方式似乎有点激进，GPT-3便采用了Few-shot的方法，给一定的标号样本，但并不多。同时，在模型规模上，由于在GPT-2实验中效果的增长并没有因为参数的增加而出现变缓的趋势，GPT-3采用了更优质的数据，参数规模直接到达**175B**，庞然大物。

## In-context learning

如此庞大的模型，在微调阶段不适合进行更新，GPT-3并没有进行微调阶段的梯度更新。在预训练过程中，数据会是多元的，那么同一类型的序列会具有相关性，提供上下文信息达到Few-shot，如图所示：

![alt In-context learning](metalearning.png)

下图为三种设置的对比，其中zero-shot是说不给任何的样本，只给出prompt来进行预测。而one-shot是给出一个有标号的样本(即翻译的样本)进行预测。Few-shot则是给出更多的样本，Few-shot也存在一定缺点，比如在预测时，模型无法利用上一次预测所抓取到的有用信息，因为并不进行训练，可能不利于上下文的任务。

![alt ](https://miro.medium.com/max/720/1*1PJi06R7QMTGBh8CdIsW8w.webp)

在模型结构方面，GPT-3只在GPT-2的基础上加入了**Sparse-Transformer**的方法。GPT-3也存在一些缺点：
- 文本生成类任务具有局限性
- AR框架，无法双向获取信息
- 均匀的预测下一个词，并没有区分重要性

GPT-3的确实印证了大力出奇迹，其后续衍生出来的各式各样的应用，带来了意想不到的效果，从而也衍生出了基于GPT-3的生态，而使GPT-3进入人们眼帘的事件便是，国外网友利用GPT-3搭建了一个文章网站，网站上的文章都是由GPT-3完成，其中一篇网站还一度霸占Hacker new榜首。BERT对NLP领域乃至整个人工智能的影响是有目共睹的，但OpenAI团队不仅仅盯着效果不放，他们似乎有着更加宏伟的蓝图，孜孜不倦浇灌，只待百花盛开。