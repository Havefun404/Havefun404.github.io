<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GPT系列技术演进</title>
    <url>/2023/01/28/GPT%E7%B3%BB%E5%88%97%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B/</url>
    <content><![CDATA[<p>GPT1-GPT3系列</p>
<span id="more"></span>
<h1>大模型演进之路</h1>
<h2 id="概览">概览</h2>
<p>随着chatGPT史诗级别的破圈，AI智能所演化出的产品如雨后春笋般，进入各行各业人士的眼中。无论是专业人士，还是b站up主，无一不被chatGPT的强大与智能所折服。而这一切的一切终究要追溯到<strong>OpenAI</strong>——这家正在改变未来的科研机构，在语言智能领域上长年累月的探索和坚持。经过一代又一代的演变之后：</p>
<ul>
<li>一方面语言模型在几乎大部分的NLP任务上都有效果。</li>
<li>另一方面根据人们的需求生成更有帮助的内容，变成众人可测，门槛更低的产品。</li>
</ul>
<p>若想要深入了解chatGPT，那就必须追寻其背后一代又一代的技术变革。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/sYdsnkRPlxk9Ig0pPOIPlG9WXRnfksUELtreHiaX4Is4Q4oIXT6CBCDxzZ2qJgIDkMZAWKImfLC4rxI2yheqTZQ/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="alt 演变之路"></p>
<h6 id="Fig1-演变之路-图片出自AI语者微信公众号">Fig1 演变之路(图片出自AI语者微信公众号)</h6>
<hr>
<h2 id="GPT：奠定AR框架路径，预训练-fine-tune先河">GPT：奠定AR框架路径，预训练+fine-tune先河</h2>
<p><strong>GPT(Generative Pre-Training)<strong>在Transformer之后提出，虽然GPT1先于举世闻名的BERT提出，尽管GPT1提出了大家目前最为熟悉的预训练+微调的模式，但奈何BERT的效果如开挂一般，横扫千军。但世人均不可否认GPT对语言智能的贡献，以及后续陆续演化出的</strong>巨无霸——GPT3</strong>。</p>
<p>受到Transformer的启发，GPT系列模型的主要架构都是Transformer中的decoder架构，通常将这种架构归类为自回归类<strong>AR(Autoregressive)</strong></p>
<blockquote>
<p>AR框架：单向获取信息，只能向前或向后来预测指定位置单词，e.g. GPT, ELMO</p>
</blockquote>
<ul>
<li>AR优点： 适合长生成类任务</li>
<li>AR缺点： 无法获取双向信息</li>
</ul>
<blockquote>
<p>AE框架：双向获取信息，向前和向后双向，e.g. BERT</p>
</blockquote>
<ul>
<li>AE优点： 利用双向信息</li>
<li>AE缺点： 预训练与微调gap</li>
</ul>
<h3 id="利用无标注数据预训练">利用无标注数据预训练</h3>
<p>模型上GPT只采用了Transformer的解码器部分，即模型只能看到当前词元和之前的信息，后面的信息都会被mask。同时，使用一个窗口大小<strong>k</strong>来预测该窗口的下一个词，目标函数如下：</p>
<p><img src="https://latex.vimsky.com/test.image.latex.php?fmt=svg&amp;val=%255Cdpi%257B150%257D%2520%255Clarge%2520L_1%2528u%2529%2520%253D%2520%255Csum_i%2520logP%2528u_i%255C%2520%257C%255C%2520u_%257Bi-k%257D%2520%255C%2520...%255C%2520u_%257Bi-1%257D%253B%2520%255Ctheta%2529&amp;dl=0" alt="alt"></p>
<p>其中，$L_1$代表第一个损失函数；$u$代表词元；k代表窗口大小；$\theta$代表模型参数。</p>
<h3 id="利用标号数据Finetune">利用标号数据Finetune</h3>
<p>在Finetune过程中，模型利用新的有标号的数据重新进行训练。将输入送给模型后，得到最终transformer模块的输出${h_l^m}$，将该部分输入到一层linear，最终放入softmax进行预测，该部分的目标函数如下：</p>
<p><img src="https://latex.vimsky.com/test.image.latex.php?fmt=svg&amp;val=%255Cdpi%257B150%257D%2520%255Clarge%2520L_2%2528C%2529%2520%253D%2520%255Csum_%257B%2528x%252Cy%2529%257DlogP%2528y%255C%2520%257C%255C%2520x%255E1%252C%255C%2520...%255C%2520%252Cx%255Em%2529&amp;dl=0" alt="alt "></p>
<p>同时，在Finetune中，作者发现将语言模型作为辅助的目标函数，有助于泛化性和更便于收敛。因此，最终优化后的目标函数变为：</p>
<p><img src="https://latex.vimsky.com/test.image.latex.php?fmt=svg&amp;val=%255Cdpi%257B150%257D%2520%255Clarge%2520L_3%2528C%2529%2520%253D%2520L_1%2528C%2529%2520%26plus%3B%255Clambda%2520L_2%2528C%2529&amp;dl=0" alt="alt "></p>
<p>而对于不同的任务，Finetune时会加上任务特定的linear层，来表示不同的input和label，如下图：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/sYdsnkRPlxk9Ig0pPOIPlG9WXRnfksUErH52Yy05S1KX3YRnYqee2QELyRm9GITTEE5V7BuWITctAGdNfgnDKg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="alt GPT Finetune"></p>
<h6 id="Fig2-GPT-Finetune-图片出自AI语者微信公众号">Fig2 GPT Finetune(图片出自AI语者微信公众号)</h6>
<p>总的来说，GPT所应用的数据(BookCorpus)和其参数规模(0.117B)与后续的模型有一定差距，并且效果被强力后辈<strong>Bert</strong>狠狠碾压，效果并没有非常出色。</p>
<hr>
<h2 id="GPT-2-更大规模，Prompt助力">GPT-2 更大规模，Prompt助力</h2>
<p>此时，已是Bert的时代，Bert声名大噪，无论是模型规模还是在下游任务中效果都产生了巨大的影响。而Bert采用的AE框架也体现出了自己的优势，毕竟效果太过显著。但OpenAI这样的公司，怎会放弃自己的老路，成为Bert的“阶下囚”，随后推出了GPT2模型。</p>
<p>GPT2主要的两个工作为：</p>
<ul>
<li>扩大规模：数据上，使用筛选了更优质的数据；模型上，将参数扩大到1.5B</li>
<li>更自然的融合方式，下面重点介绍这点。</li>
</ul>
<h2 id="zero-shot-与-Prompt">zero-shot 与 Prompt</h2>
<p>无论是GPT，还是Bert都采用了预训练+微调的方式。但：</p>
<ul>
<li>预训练的过程和微调的下游任务之间可能存在一定的gap，无法完全发挥预训练模型的优势；</li>
<li>同时微调的时候也需要将大模型重新训练，耗时耗力；</li>
<li>并且在微调时需要收集带标签的数据。</li>
</ul>
<p>为了克服这些困难，GPT2利用<strong>Prompt</strong>的方式，将下游的任务加到预训练模型中进行预测，从而彻底丢弃微调训练的过程，进而实现<strong>zero-shot</strong>。<br>
Prompt具体加入方式，举例来说，在机器翻译中，将句子表示成：<br>
<u><strong>Translate it</strong></u> ,<u>I love computer</u> ,<u>我爱计算机</u> 开头的部分就为Prompt。</p>
<p>引入Prompt起到作用的原因，可以大致理解为：如果模型足够强大就有能力识别出要求；其次数据中可能存在类似prompt这样的信息。尽管GPT2在不少任务上有效果，但相比Bert而言并没有太大的提升。虽然GPT2并没有激起多大的风浪，但其更自然的融合方式，这样的思想为后续的GPT3以及如今的chatGPT打下来重要的基石。</p>
<hr>
<h2 id="GPT-3-大力出奇迹">GPT-3 大力出奇迹</h2>
<p>GPT-2的zero-shot方式似乎有点激进，GPT-3便采用了Few-shot的方法，给一定的标号样本，但并不多。同时，在模型规模上，由于在GPT-2实验中效果的增长并没有因为参数的增加而出现变缓的趋势，GPT-3采用了更优质的数据，参数规模直接到达<strong>175B</strong>，庞然大物。</p>
<h2 id="In-context-learning">In-context learning</h2>
<p>如此庞大的模型，在微调阶段不适合进行更新，GPT-3并没有进行微调阶段的梯度更新。在预训练过程中，数据会是多元的，那么同一类型的序列会具有相关性，提供上下文信息达到Few-shot，如图所示：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/sYdsnkRPlxk9Ig0pPOIPlG9WXRnfksUE5CLassHKNkWjxdOqIjsy51s0mXuySoyL8EKP5Oicm58yzqueDuI3oaA/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="alt In-context learning"></p>
<h6 id="Fig3-上下文context-图片出自AI语者微信公众号">Fig3 上下文context(图片出自AI语者微信公众号)</h6>
<p>下图为三种设置的对比，其中zero-shot是说不给任何的样本，只给出prompt来进行预测。而one-shot是给出一个有标号的样本(即翻译的样本)进行预测。Few-shot则是给出更多的样本，Few-shot也存在一定缺点，比如在预测时，模型无法利用上一次预测所抓取到的有用信息，因为并不进行训练，可能不利于上下文的任务。</p>
<p><img src="https://miro.medium.com/max/720/1*1PJi06R7QMTGBh8CdIsW8w.webp" alt="alt "></p>
<p>在模型结构方面，GPT-3只在GPT-2的基础上加入了<strong>Sparse-Transformer</strong>的方法。GPT-3也存在一些缺点：</p>
<ul>
<li>文本生成类任务具有局限性</li>
<li>AR框架，无法双向获取信息</li>
<li>均匀的预测下一个词，并没有区分重要性</li>
</ul>
<p>GPT-3的确实印证了大力出奇迹，其后续衍生出来的各式各样的应用，带来了意想不到的效果，从而也衍生出了基于GPT-3的生态，而使GPT-3进入人们眼帘的事件便是，国外网友利用GPT-3搭建了一个文章网站，网站上的文章都是由GPT-3完成，其中一篇网站还一度霸占Hacker new榜首。BERT对NLP领域乃至整个人工智能的影响是有目共睹的，但OpenAI团队不仅仅盯着效果不放，他们似乎有着更加宏伟的蓝图，孜孜不倦浇灌，只待百花盛开。</p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/11/02/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>hexo 搭建部署</title>
    <url>/2022/11/02/hexo-%E6%90%AD%E5%BB%BA%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>Hexo 操作指南<br>
<a href="https://hexo.io/zh-cn/docs/">Hexo的部署文档</a></p>
<span id="more"></span>
<h1>基础命令</h1>
<h2 id="新建文章">新建文章</h2>
<blockquote>
<p>hexo new [layout] “title”</p>
</blockquote>
<p>新建文章，如果不设置layout会自动选择_config.yml中的default. 如果title中包含空格，要使用<strong>引号</strong>扩充.</p>
<ul>
<li>可以使用-p or --path 来自定义文章的路径，后面加文件路径即可.</li>
</ul>
<h2 id="部署文章">部署文章</h2>
<ul>
<li>生成静态文件：hexo g; 加-d or --deploy 直接部署</li>
<li>发布草稿：hexo publish [layout] “filename”</li>
<li>启动服务器：hexo server</li>
<li>部署文章：hexo d; 加-g or --generate 部署前先生成静态文件</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>NLP Lecture</title>
    <url>/2023/03/06/NLP-Lecture/</url>
    <content><![CDATA[<p>NLP学习</p>
<span id="more"></span>
<h1>Text Preprocessing</h1>
<h3 id="Defination">Defination</h3>
<ul>
<li>word: sequence of characters</li>
<li>sentence: sequence of words</li>
<li>Corpus(语料): collection of documents</li>
<li>word token: each word</li>
<li>word type: distinct word</li>
</ul>
<blockquote>
<p>为什么做preprocessing？<br>
语言是<strong>compositional</strong>,人类理解语言是将文本分解成句子和词，电脑也应该这样理解。</p>
</blockquote>
<h1>Prepocessing steps</h1>
<ul>
<li>Remove unwanted information</li>
<li>Sentence segmentation: 将document分解为sentence</li>
<li>Word tokenisation: 将句子分解为word token</li>
<li>Word normalisation: 将word进行标准化</li>
<li>Stopword removal: 移除不想要的word</li>
</ul>
<h2 id="Sentence-segmentation">Sentence segmentation</h2>
<ul>
<li>Naive method: 利用标点符号(punctuation)分割; 但是像句号(periods)在英文中常被用来缩写(abbreviations)</li>
<li>Second way: 利用正则表达式(regex)结合大写字符(capital);但很多缩写会跟随名字，无法查分开</li>
<li>Better way: 构建词汇表(lexicon);但很难含括全部的名字和缩写</li>
<li>State-of-the-art:利用ML来进行预测</li>
</ul>
<blockquote>
<p>二分类做句子分割<br>
查看每一个period，来判断是否为一句话的结尾；可以利用决策树或逻辑回归。<br>
特征可以包括: 句号前后的word; word的shape;词性tag</p>
</blockquote>
<h2 id="Word-Tokenisation">Word Tokenisation</h2>
<blockquote>
<p>Word Tokenisation是nlp任务中重要的环节，因为我们可以通过分析文本中的单词，来<br>
解释文本的含义。<br>
Naive method就是按照字符串进行分割，但是存在很多的问题：</p>
</blockquote>
<ul>
<li>在英文中，存在缩写，连字符，数字，日期，多词组合等情况。</li>
<li>在中文中，没有空格可以将词分割开；同时，英文单词可能对应多个中文词；</li>
<li>在德文中，很长的词，需要复合分离器(compound splitter)<br>
更进阶的方法可以先构建词库，然后利用<strong>MaxMatch</strong>算法；该算法的思想就是贪心地匹配最长的词在词表中。一直增加字符与词表中的词进行匹配，直到找不到对应的词。<br>
<img src="https://lh3.googleusercontent.com/FN6Y-8wHXFG4CzugyY5ZSYftft-MKYNC1tVudQfFFbxCIpt3WIWAPjVrEtVnXyCzKYLVv04dYvhL2zWckyU2cUnNhAgoMLzHNxgcbg9p" alt="example"></li>
</ul>
<h3 id="Byte-pair-encoding-BPE">Byte-pair encoding(BPE)</h3>
<p>BPE算法属于<strong>subword toeknisation</strong>。该类方法可以解决unknown word的情况。一般NLP算法会学习一些关于语料库的facts通过训练集，然后利用这些facts来在test集合做相关决策。但如果test集合中出现了train中未曾有过的词，那系统可能就不知道如何处理了。<br>
而这样的分词方法可以自动的归纳包含小于单词的token集合。这些supword也会包含一些类似-est/-er的后缀词，这样一些从未见过的词，就可以通过不同的subword的组合来构成，从而可以解决上述的问题。该类方法主要有：BPE,<a href="https://doi.org/10.18653/v1/P18-1007">Unigram language modeling</a>, <a href="https://doi.org/10.18653/v1/D18-2012">wordpiece</a>本节重点介绍BPE算法.</p>
<p>BPE算法的核心思想是：<strong>迭代的合并高频率字符对</strong>。有以下优点：</p>
<ul>
<li>Data-informed tokenisation</li>
<li>Works for different language</li>
<li>Deal better with unknown words<br>
该算法通常是在单词内部进行，所以输入的语料会先被空格分割开以此来给出一组字符串，每一个对应一个单词的字符，再加上特殊的结尾符合“—”,以及计数。<br>
<img src="https://lh3.googleusercontent.com/otanio14Qm6Lmgn65r94RlPqlctGGlhNdKI43fYmK0eqBUOEkGP_Hw1toUrKrEBPLiiBDQhT4VrbbvPb7QYkZ5ApejHKWxoD7CU_g4Hk5g" alt="example"></li>
</ul>
<ol>
<li>首先会构建一个初始的词汇表(vocabulary)，该词汇表只有单独的字符来组成。</li>
<li>然后检查词汇表，选择最频繁相邻的两个字符(例如A,B)，将新合并的字符AB添加到词汇表中，并且在语料中替换每个相邻的AB，得到带有新的AB的语料</li>
<li>继续计数和融合，不断构造新的更长的字符串，直到<strong>k</strong>次融合后，构建k个新的词汇。一般k是算法的参数。<br>
<img src="https://lh3.googleusercontent.com/L0CMesVV_Y9pFnyo2OYKxAIHMPCoIurVaBYTZfeUiG34dO32GYkuCjH5ffGh2Naa5vZ2a-QPVhFPBAYmGWEttntLCS29lqgps5idhMxqoA" alt="example"><br>
在实际操作中，BPE算法可能会运行上千次的融合操作来构建出一个巨大的语料；大多高频率出现的词，会在语料中被表示为完整的词；一些比较罕见的词会被分解成subword。最坏的情况下，在test集中的unknown word会被分解成单独的字母</li>
</ol>
<h2 id="Word-Normalisation">Word Normalisation</h2>
<p>为了更好的进行处理，需要将word进行标准化的操作，将word的表示统一。以此来达到<strong>减少词数量</strong>和<strong>将word匹配到相同类型</strong>的目的。当然，这样的操作可能会丢失拼写信息，在机器翻译，情感分析，信息提取中不太适用。而对于信息检索等任务比较适用。例如<strong>Inflectional Morphology</strong>(不同词形)会产生很多语法变种，包括名词，动词，形容词。其他语言例如法语的现象更多。<br>
除此之外，还存在<strong>Derivational Morphology</strong>(衍生词汇)的情况，它会产生很多不同的词；英文中包含两类：</p>
<ul>
<li>suffixes(后缀): 往往会改变词汇的类别(lexical category)，例如write-&gt;writer</li>
<li>prefixes(前缀): 往往会改变含义而不改变类别，例如write-&gt;rewrite</li>
</ul>
<h2 id="Lemmatisation——词形还原">Lemmatisation——词形还原</h2>
<p>Lemmatisation可以用来判断两个词是否有相同的词根，通过移除任意的变形来达到还原。这就需要<strong>lexicon of lemmas</strong>(词典)来辅助。</p>
<h2 id="Stemming——提取主干">Stemming——提取主干</h2>
<p>移除所有的后缀，只留下主干，这种方法会比lemmatisation更低的词汇稀疏性，在信息检索中非常流行。但并不总是可解释的。</p>
<h3 id="两种方法的讨论">两种方法的讨论</h3>
<ol>
<li>不同点<br>
Stemming方法是将后缀移除，这样的方法在某些场景很合适，但并不总是这样。<br>
Lemmatisation方法是需要考虑词的形态分析，所以需要构建相关的词典。lemma是所有变形体的根本，而stem不一定是，所以构建的词典是关于lemma的。<br>
Stemming会导致<strong>更低的词汇稀疏性</strong>：因为stemming的方法会导致一些单词被错误地截断为相同的基本形式，增加歧义，减少准确，降低词汇的稀疏性(因为有些词出现的频率增加了，词汇稀疏性便降低)。而lemmatisation不仅考虑形态，还会考虑上下文和词性等信息，减少歧义，更高稀疏性，更高准确，但是计算成本较高，也需要更多的时间和成本。</li>
<li>如何选择<br>
一般会根据任务情况选择其中的一种方法。Stemming常常用于信息检索任务，大多其他的任务都是使用lemmatisation。因为Stemming之后，会丢失大量的和token相关的信息，虽然lemmatisation也会丢失一些，但总体较少，对于大多数NLP任务，以丢失更多单词信息为代价来换取更低的词汇稀疏性是不值得的。</li>
</ol>
<h3 id="The-Porter-Stemmer">The Porter Stemmer</h3>
<p>在英文中流行的算法，应用重写的规则，首先去除词形变化的后缀，再去除衍生词的后缀。规则包含：</p>
<ul>
<li>c(小写c): 代表辅音(consonant) 例如: b, c, d</li>
<li>v(小写v): 代表元音(vowel)例如: a, e, i, o, u</li>
<li>C(大写C): 代表sequence of consonant 例如: s, ss</li>
<li>V(大写V): 代表sequence of vowels 例如: o, oo</li>
</ul>
<p>一个单词基本是以下四种形式中的一种：</p>
<ul>
<li>CVCV…C</li>
<li>CVCV…V</li>
<li>VCVC…C</li>
<li>VCVC…V<br>
可以进一步缩写表示为:[C](VC)^m[V]; m为<strong>measure</strong>;例子如下图：<br>
<img src="https://lh3.googleusercontent.com/pRAhDxgYcimHpcsnk6imyVTwNaG3ppvUy76Y4aYSB-AAYL7HGyhXp3YFIiF3v5hks0HMjgqLKTu5M_sJVywu7idcGc6f76GuMnikOQhN" alt="example"><br>
该算法的规则形式为：(condition)S1 -&gt; S2；condition中会给出关于m的限制，计算给出词的m时，注意：要<strong>取掉S1来计算</strong>，同时要利用<strong>最长匹配</strong>的原则来匹配S1；示例如下图<br>
<img src="https://lh3.googleusercontent.com/rEl1RcfnDPiiMmuUxVL4ns5aqayONX3pcym3eUs09sue1j08gbv6Hz1vu7k673jIubEOhdjx3xtehaJwIrQ0pJ-udaJVIpdCnDLAGp48" alt="example"></li>
</ul>
<p>词形还原的顺序：</p>
<ul>
<li>首先处理plurals(复数)和inflectional(时态变形)</li>
<li>然后处理derivational(衍生词)</li>
<li>最后再做一些清洗操作</li>
</ul>
<h1>Stopword Removal</h1>
<p>Stop word是一系列需要从文档中移除的词；特别是bag-of-words表示时候，但不太适合sequence比较重要的情况。一般stop word有：</p>
<ul>
<li>all closed-class or function，例如the, a, of, for…</li>
<li>高频出现的词语</li>
<li>或通过工具来筛选，比如NLTK等<br>
Final word是最终保留下来的词，对下游的应用有重要的影响。</li>
</ul>
<p>[1] <a href="https://web.stanford.edu/~jurafsky/slp3">https://web.stanford.edu/~jurafsky/slp3</a></p>
]]></content>
  </entry>
</search>
